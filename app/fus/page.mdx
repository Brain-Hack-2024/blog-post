import Figure from "../components/Figure"
import Video from "../components/Video"
import Image from "next/image"
import ExpandableSection from "../components/ExpandableSection"
import Annotation from "../components/Annotation"


{/* Images */}
import attenuationImg from "/public/fus/attenuation.png"
import aberrationCorrectionImg from "/public/fus/aberration-correction.png"
import attenuationJigImg from "/public/fus/attenuation-jig.jpg"
import skullRegionsImg from "/public/fus/skull-regions.png"
import skullImg from "/public/fus/skull.png"
import virtualSensorsImg from "/public/fus/virtual-sensors.png"
import attenuationMeasurementsImg from "/public/fus/attenuation_measurements.png"
import phantomImg from "/public/fus/phantom.png"
import pumpImg from "/public/fus/pump.png"
import bannerImg from "/public/fus/banner.png"
import bannerMobileImg from "/public/fus/banner-mobile.png"
import fusPicImg from "/public/fus/fus_pic.png"
import planeWaveImg from "/public/fus/plane_wave.png"



export const metadata = {
  title: "Functional ultrasound through the skull",
  description: "We show new results for making functional ultrasound work through the skull."
}

export const R2_URL = "https://pub-9ded51b796ab46ffb6c70940cf4b6be4.r2.dev"

# Functional ultrasound through the skull
<div className="author">
By: [Raffi Hotter](https://rhotter.vercel.app/), [Vincent Huang](https://www.vvhuang.com/), [Brian Machado](https://twitter.com/sincethestudy), Thomas Ribeiro, and [Anson Yu](https://ansonyu.me/)
</div>

[Functional ultrasound imaging](https://en.wikipedia.org/wiki/Functional_ultrasound_imaging) (fUSI) is an awesome new technology for imaging brain activity. Its spatial resolution can be even better than fMRI. The hardware to run ultrasound can be made [compact and cheap](https://www.butterflynetwork.com/).

Could we possibly use it to make a brain-computer interface? Similar to the [acousto-electric effect](/ae), fUSI could have **10,000x** more channels than state-of-the-art EEG.

<Figure caption="Functional ultrasound in a rat brain (from [Macé et al., 2011](https://www.nature.com/articles/nmeth.1641))" centered>
<Video src={`${R2_URL}/fus.mp4`} />
</Figure>


So far, all fUSI experiments involve opening up the skull or replacing it with an [acoustically-transparent window](https://www.science.org/doi/10.1126/scitranslmed.adj3143). Not the greatest if you want a non-invasive brain-computer interface. Could we do functional ultrasound with the skull intact?

When we asked around about doing functional ultrasound with the skull intact, we were told it was impossible. There are two big challenges in it ultrasound work through the skull:

1. High frequency ultrasound waves get distorted when they pass through the skull. If you use traditional ultrasound beamforming and don't account for this distortion, you're bound to fail.

<Figure caption="A 1 MHz ultrasound wave propagating through a skull model, obtained via CT. A 2D solver with no absorption was used for simplicity. ([Jupyter notebook](https://github.com/Brain-Hack-2024/transcranial-ultrasound/blob/main/skull-propagation/skull_propagation.ipynb))">
<Video src={`${R2_URL}/pressure_animation_1.0MHz.mp4`} />
</Figure>


2. The skull attenuates ultrasound, so there might not even be enough signal once the ultrasound has passed through the skull and back.

[People](https://www.cambridge.org/core/books/diagnostic-ultrasound/E49EEE09C4081D1EFE1DC457C7D7973C) [often](https://www.sciencedirect.com/book/9780122228001/physical-properties-of-tissues) [quote](https://pubmed.ncbi.nlm.nih.gov/690336/) 22 dB/cm/MHz attenuation of ultrasound. Decibels are a logarithmic scale, so with 1.4 cm of roundtrip skull distance, and typical fUSI frequencies of 10 MHz, this would be 14 orders of magnitude of attenuation! In physics, there's a word for 14 orders of magnitude of attenuation. It's called zero, i.e., you will measure nothing.

But where did the 22 dB/cm/MHz attenuation number come from? We were skeptical of the methods used to measure this.

We ran a 10-day hacksprint to try to answer if functional ultrasound through the skull was possible.

Two main results:

1. Attenuation through the skull is actually not as high as currently reported!

2. We can de-abberrate the signal after it interfaces with the skull. Code and algorithm shared!

<div className="w-full sm:w-screen sm:-ml-[calc(50vw-50%)] flex justify-center hidden sm:flex">
  <Image src={bannerImg} alt="Banner image showing functional ultrasound through the skull" className="w-full max-w-6xl" />
</div>

<div className="w-full flex justify-center sm:hidden -px-8">
  <Image src={bannerMobileImg} alt="Banner image showing functional ultrasound through the skull" className="w-full" />
</div>

### What is functional ultrasound?

Before anything else, a quick primer on functional ultrasound. Functional ultrasound measures changes in blood flow or volume in the brain. Blood changes are correlated with neural activity — more neurons firing means more energy consumed, which means more blood that will flow to deliver more energy.

Functional ultrasound measures blood volume by sending short pulses of ultrasound into the brain. Because blood cells have a different acoustic impedance than the background, the blood cells scatter the sound waves. Ultrasound transducers then read the scattered sound waves and form an image of the scattering.

<Figure>
<Image src={fusPicImg} alt="Diagram illustrating functional ultrasound imaging process" />
</Figure>


### What frequency to use?

The first design question for an ultrasound imager is what frequency to use. Higher frequencies attenuate ultrasound more — the typical 10 MHz ultrasound would attenuate the signal to well below the noise levels of ultrasound transducers.

You might then think to use as low of a frequency as you can. But there's a problem with that, too: as you lower the frequency, the amount of scattering from red blood cells decreases and you get less signal back. The physical reason for this is that the scattering is in the [Rayleigh regime](https://en.wikipedia.org/wiki/Rayleigh_scattering) (the red blood cells are a lot smaller than the wavelength of ultrasound), and the scattering in this regime scales with the fourth power of frequency.

So what's the optimal frequency? We modeled the ultrasound attenuation through the skull and the Rayleigh scattering from the blood and asked what frequency gives the most signal back.

<Figure caption="Ultrasound signal drop through bone for blood cells. This combines both reduced Rayleigh scattering and bone attenuation effects. This uses an attenuation of 10 dB/cm/MHz through 0.7 cm of skull each way. The attenuation is given relative to 5 MHz functional ultrasound without the skull, which is the lowest frequency we've seen people do functional ultrasound ([Jupyter notebook](https://github.com/Brain-Hack-2024/transcranial-ultrasound/blob/main/frequency-selection/main.ipynb))">
  <Image src={attenuationImg} alt="Graph showing ultrasound signal drop through bone for blood cells" />
</Figure>


It turns out that 1.25 MHz gives the least signal drop for an attenuation of 10 dB/cm/MHz. So we use 1-2 MHz as a first guess.<Annotation>The signal drop isn't the only metric that matters. Another relevant metric is contrast: higher frequencies will give higher contrast between the Rayleigh scattering blood cells and the background.</Annotation>


### Skull CT Scan

The thing that nobody tells you is that you can buy a real human skull online (shoutout to [skullsunlimited.com](https://skullsunlimited.com)). We did that, and then CT scanned it.


<Figure>
<Image src={skullImg} alt="Photo of a human skull used for CT scanning"/>
</Figure>


Here's what the CT scan looks like. We thought this was really cool! You can access the scan [here](https://drive.google.com/file/d/1_DwyJauas5qvFQ3T0cpL_5P-mgHvKkRE/view?usp=sharing).

<Figure>
<Video src={`${R2_URL}/ct.mp4`}/>
</Figure>


### De-aberration

To correct for the distortion of ultrasound through the skull, we developed our own skull de-aberration algorithm.

Aberrations in the body come from changes in the speed of sound<Annotation>Changes in density and even the stiffness of the material can also affect wave propagation, but those are less pronounced than changes in speed of sound in tissue.</Annotation>, similar to how light bends in Snell's law. There are two problems here

1. Bone has a much higher speed of sound (\~2800 m/s) than the brain (\~1540 m/s)

2. Part of the skull is actually porous, filled with marrow. There's a big change in the speed of sound of the bone (\~2800 m/s) and the marrow (\~1540 m/s).


<Figure caption="Different parts of bone (source: [Anatomy and Physiology](https://openstax.org/books/anatomy-and-physiology/pages/6-3-bone-structure))" centered>
<Image src={skullRegionsImg} alt="Diagram showing different parts of bone structure"/>
</Figure>

The problem is that traditional ultrasound imaging assumes that the medium's speed of sound is the same everywhere.

The engine of our algorithm is a wave propagation simulator. It takes in a speed of sound map and source waves, and it simulates waves propagating through the medium.

<Figure>
<Video src={`${R2_URL}/waves.mp4`} />
</Figure>


We use a CT scan of the skull to estimate the skull's speed of sound map. Then, our algorithm considers virtual sensors below the skull.<Annotation>To go from the CT scan to a speed of sound map, we binarize the CT skull into tissue/marrow and bone. We set bone voxels to 2,800 m/s and the rest to 1,540 m/s.</Annotation>

<Figure>
<Image src={virtualSensorsImg} alt="Diagram illustrating virtual sensors below the skull" />
</Figure>

We use the wave solver to find a transformation between the data recorded at the actual sensors that are above the skull and the data that would have been recorded if they were at the virtual positions below the skull. Then, we use this transformation to act as if the sensors were below the skull, and apply standard delay-and-sum beamforming. The code for reproducing our results can be found in [`recon_skull.ipynb`](https://github.com/Brain-Hack-2024/transcranial-ultrasound/blob/main/notebooks/recon_skull.ipynb) in our GitHub repository.

<ExpandableSection title="How do we find the transformation?">

To find the transformation, we play a trick with time-reversing waves. If the sensors retransmit a time-reversed version of what they measured, you can, in some sense, “rewind” the waves.

There's two parts to the transformation: on transmit and on receive.

On transmit, we simulate the virtual sensors sending a plane wave, and we measure the signal at the real sensors. Then, in real life, we get the sensors to send a time-reversed version of what they measured in simulation. This should produce a plane wave at the virtual sensor location.

<Figure>
<Image src={planeWaveImg} alt="Diagram showing plane wave propagation through the skull"/>
</Figure>

On receive, we take the data measured by our real transducers in reality, and propagate the time-reversed version through the wave simulator, while recording at the virtual sensors. We use the virtual sensor data for the subsequent beamforming.
</ExpandableSection>

We tested our algorithm in a 2D simulation, with a cross-section of our CT-scanned skull. We placed a small scatterer below the skull, and the goal was to reconstruct it.

<Figure fullWidth>
<Image src={aberrationCorrectionImg} alt="Comparison of regular beamforming and de-aberration algorithm results" />
</Figure>

Regular beamforming completely fails, but our algorithm can correctly capture the location of the spot.<Annotation>After the hack, we found out about an [existing approach](https://ieeexplore.ieee.org/abstract/document/10307026) to skull de-abberation, but it doesn't seem to work as well as our method ([see section 2c of our Jupyter notebook](https://github.com/Brain-Hack-2024/transcranial-ultrasound/blob/main/notebooks/recon_skull.ipynb)).</Annotation>

We also started to test our approach on real data. We have some preliminary results that show that we can de-abberate through a 3d printed squiggly material (meant to emulate the skull), but we need to do some more testing.

In translating this approach to humans, we won't have access to a CT scan. But, perhaps an MRI would suffice. Or, maybe you could use machine learning to bypass de-aberration altogether.


### Attenuation measurements

Now that we have a potential way to de-aberrate through the skull, we wanted to know if there would even be enough signal after the ultrasound passes through the skull and back. Ultrasound transducers have noise that's about $1 \text{ mPa}/\sqrt{\text{Hz}}$, which is \~1 Pa for a \~1 MHz bandwidth ([ref](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7407969/)). So we need to make sure the signal we receive is above that.


<ExpandableSection  title="Why not just shoot more ultrasound?">

Unfortunately, there's a safety limit. Since the head absorbs ultrasound, it heats up a little bit. The safety limit imposed by the International Electrotechnical Commission (IEC) is 2ºC, so we need to make sure not to surpass that.

</ExpandableSection>

To estimate how much signal we'd get back, we needed to know how much the skull attenuates ultrasound. You'd think the literature would have an answer to this, but different sources report widely different answers, from [8.3 dB/cm/MHz](https://pubmed.ncbi.nlm.nih.gov/16829322/) to [22 dB/cm/MHz](https://www.cambridge.org/core/books/diagnostic-ultrasound/E49EEE09C4081D1EFE1DC457C7D7973C). For some reason, 22 dB/cm/MHz is the number people will tell you in conversation.

We decided to measure it ourselves. We built a little mechanical jig to hold an ultrasound probe on one side of the skull to transmit and a small hydrophone on the other to receive.

<Figure>
<Image src={attenuationJigImg} alt="Photo of the mechanical jig used for attenuation measurements" />
</Figure>

<ExpandableSection title="More technical details">

We had the probe send a plane wave through the skull and we measured the pressure levels at the hydrophone. Our jig allowed us to place the hydrophone in 9 different spots, so that we can be sure to capture as much of the wave as possible. We took 9 measurements at two different locations on the head (occipital and temporal), and also had a control without the skull. We used continuous wave transmission at multiple frequencies: 1 MHz, 2 MHz, and 3 MHz.

We used a [Phillips P4-1](https://www.ebay.com/itm/115853090479) probe that we found on Ebay. We chose it because it was the transducer that operated at the lowest frequencies we could find (1-4 MHz bandwidth). We measured the signal using an [Onda HNR 500](https://www.ondacorp.com/hnr-needle-hydrophone/) connected directly to a standard oscilloscope. We drove the transducer using a Verasonics Vantage 64 system. The skull was degassed to remove air bubbles (which would otherwise cause lots of scattering) and it was placed in distilled water.
</ExpandableSection>

<Figure>
<Video src={`${R2_URL}/skull_jig.mp4`} sound controls />
</Figure>


A few hours before the hacksprint ended, we rushed to get some measurements in. We only had time to take measurements at two jig locations on the head (with 9 hydrophone spots at each location). This is the attenuation we measured:

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdVQLRpMruD3GcXcEV3uhr9TDBUm4pfSxRCaotLB4aeJOQxQzLSdjncSBE_Y8oAvq2MTdw82_AttdTbsXttVqCWUd12lq35LiqsRF9MbNmwu7UX5X8ogQie4sRnegasRVfocWH9yQKw22syljPWx4MkUt5f?key=pBPggEzLFBh2YWTQGQnEtw)

The attenuation we measured was 11.18 dB/cm/MHz, which was on the lower side of what we saw in the literature.

<ExpandableSection title=" How did we compute attenuation?">

First, the data was preprocessed by bandpass filtering the data around the reference frequency. Then, for each jig location, we computed the total power as the squared measurements summed over all 9 locations. Then we divided the total power at each jig location by the total power with no skull. We measured the thickness of the skull at each jig location with a digital caliper. See this [Jupyter notebook](https://github.com/Brain-Hack-2024/transcranial-ultrasound/blob/main/skull-attenuation-analysis/main.ipynb) for our data analysis.

</ExpandableSection>

### Why are our results different?

Since we only had time to measure two jig locations (+ we sketchily removed a nonsensical outlier measurement), you should take our measurements with a grain of salt. But upon digging into the literature, we found big problems with using the [Fry 1978](https://pubmed.ncbi.nlm.nih.gov/690336/) paper that measures 22 db/cm/MHz attenuation.

Firstly, there is no mention in the paper of degassing the skull. Without degassing, there could be many air bubbles formed in the pores, which will scatter ultrasound heavily (since there is a huge mechanical impedance mismatch between air and water).

Secondly, they use focused transmission, instead of sending plane waves. The skull will spread out the focus, so if you don't sample enough, you'll mistake spreading for attenuation.

The only good reference we could find for attenuation across frequencies was a 2006 paper by White. They degas the skull and use plane waves. They claim to measure 8.53 dB/cm/MHz, but when we tried [reproducing their analysis](https://docs.google.com/spreadsheets/d/1A63-Qzqo9rgjiS1kin3i3dF24EzdTXvK-ylPrlOjii8/edit?gid=0#gid=0) using their data, we got 11.9 dB/cm/MHz. This is very close to what we measured!

An important point is that we measured attenuation, which is different from absorption. Attenuation also includes things like scattering and reflections. But it's absorption that really limits us, not attenuation, since absorption is what contributes to heating in the head. Our attenuation measurements serve as an upper bound on absorption.

It could be that the absorption is a lot lower than the attenuation. [Pinton et al](https://pubmed.ncbi.nlm.nih.gov/22225300/) find that with a 1 MHz pulsed source, only 2.7 dB/cm of the measured 13.3 dB/cm was due to absorption.


### Could fUSI through the skull work?

So back to the key question: does fUSI  through the skull get a high enough signal-to-noise ratio (SNR)?

If we use 11 dB/cm/MHz as the attenuation value, we'd expect a signal drop of about 40 dB, or equivalently, 100x in pressure, relative to below-skull functional ultrasound (see the earlier plot) If we use 2.7 dB/cm/MHz, we get a signal drop of 22 dB, or 10x in pressure.

Is that too low? We're not sure yet. It depends on the pressure changes that are typically seen in regular functional ultrasound. Unfortunately, we couldn't find that in the literature (if you know this number, please let us know). If the changes in regular functional ultrasound are 100 Pa, then we'd expect to see changes of about 1-10 Pa in functional ultrasound through the skull. This is 1-10 times larger than the noise floor of a transducer (\~1 Pa). And we can use a functional ultrasound trick, called [coherent compounding](https://pubmed.ncbi.nlm.nih.gov/19411209/), to increase the SNR further. Note, this is the SNR in the sensor domain, but what we really care about is the SNR in the image domain.


### Doppler testbed

We also built a testbed to test fUSI outside of simulation. We wanted it to have properties similar to those of the brain. Previous [work](https://pubmed.ncbi.nlm.nih.gov/19101073/) showed that tofu is desirable as a phantom material, both because it is fast to get and because it has similar physical properties (density, speed of sound) as soft tissue.

Because functional ultrasound works by detecting the movement of blood cells in blood vessels, we also needed a way to emulate blood vessels in the phantom. We accomplished this by building our own pump system.


#### Syringe Pump

We built our own syringe pump. Blood travels at around [\~10 mm/s](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6610068/) in small vessels in the brain, so we wanted our pump to have similar speeds. Initially we planned on using a peristaltic pump, however we could not source one that had a small enough volumetric flow rate, and at slow speeds peristaltic pumps flow rate is not consistent.

<ExpandableSection title="Technical details">

We purchased a premade stepper motor driven linear actuator, and mounted it to a base plate. Normal sterile syringes of different sizes were used and custom mounts for each one could be swapped in; this along with adjusting the actuator speed in our Arduino code allows us to vary the volumetric flow rate. We also had various sizes of PTFE and silicone tubing to simulate different sized veins. We used [Ultrasound Refill Fluid](https://anatomywarehouse.com/ultrasound-refill-fluid-a-108447?srsltid=AfmBOopmF2Y_8Zg8SybmpeO0B7QzFVYJSc8CizoTEapDtcQYBL2WcSgQ), which has similar acoustic properties to blood.

The stepper motor was controlled by a TB6600 controller, and step signals were sent by a Teensy4.1 running this [program](https://github.com/Brain-Hack-2024/transcranial-ultrasound/blob/main/syringepump/teensycontrol.ino) we wrote to easily adjust flow rate, typically 1-10mm/sec, analogous to the rate at which blood cells in smaller blood vessels and capillaries move.
</ExpandableSection>

<Figure>
<div className="flex flex-row gap-4">
  <div className="w-1/2">
    <Image src={phantomImg} alt="Photo of the tofu phantom used for testing" />
  </div>
  <div className="w-1/2">
    <Image src={pumpImg} alt="Photo of the custom-built syringe pump" />
  </div>
</div>
</Figure>

### We have AE at home

Check out our other post, [We have AE at home](/ae)!


### Acknowledgements

A big thank you to:
- Protocol Labs for sponsoring the hacksprint
- [Hunter Davis](https://twitter.com/huntercoledavis), [David Garrett](https://twitter.com/david_c_garrett), [Sumner Norman](https://twitter.com/SumnerLN), [David Maresca](https://twitter.com/DMarescalab), and [Hari Kumar](https://twitter.com/Harie1897) for technical advice
- [Leo Zaroff](https://twitter.com/leozaroff) for lending us his hydrophone and for advice

### Code
The whole project is open source. You can find the code on our [GitHub repository](https://github.com/brain-hack-2024/transcranial-ultrasound/).